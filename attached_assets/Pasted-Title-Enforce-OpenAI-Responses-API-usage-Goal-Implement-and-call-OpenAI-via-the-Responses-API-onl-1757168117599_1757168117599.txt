Title: Enforce OpenAI Responses API usage

Goal: Implement and call OpenAI via the Responses API only. Mirror parameter names and response shapes exactly. No chat.completions.

Source of truth (must consult before coding)

OpenAI Responses API:
- Overview: https://platform.openai.com/docs/api-reference/responses
- Create endpoint: https://platform.openai.com/docs/api-reference/responses/create
- Streaming guide: https://platform.openai.com/docs/api-reference/responses-streaming
- API intro: https://platform.openai.com/docs/api-reference/introduction
Official JS/TS SDK:
- https://github.com/openai/openai-node  (library usage + examples)


Compliance rules

Use client.responses.create(...) from the official JS/TS SDK. Do not use chat.completions. The SDK docs state the primary API is the Responses API; follow that. 
GitHub

Only use parameters and shapes shown in the docs. If uncertain, paste the exact snippet you’re following from the pages above. 
OpenAI Platform
+1

For live output, implement the streaming pattern per the official streaming docs. 
OpenAI Platform
+2
OpenAI Platform
+2

Minimal server template (TypeScript)

// src/lib/openai.ts
import OpenAI from "openai";
export const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

// Simple text generation via Responses API
export async function generateText(input: string) {
  const res = await openai.responses.create({
    model: process.env.OPENAI_MODEL || "gpt-5",
    input
  });
  // Access text per current docs shape; keep this mapping in one place.
  const output = res.output_text ?? res.output?.[0]?.content?.[0]?.text ?? "";
  return output;
}


Streaming template (server-sent events)

// src/routes/api/respond.ts
import type { Request, Response } from "express";
import { openai } from "../lib/openai";

export async function respond(req: Request, res: Response) {
  res.setHeader("Content-Type", "text/event-stream");
  res.setHeader("Cache-Control", "no-cache");
  res.flushHeaders();

  const stream = await openai.responses.stream({
    model: process.env.OPENAI_MODEL || "gpt-5",
    input: req.body?.input || "Hello",
  });

  for await (const event of stream) {
    // Forward token/text chunks; shape per streaming docs.
    if (event.type === "response.output_text.delta") {
      res.write(`data: ${JSON.stringify({ delta: event.delta })}\n\n`);
    }
    if (event.type === "response.completed") break;
  }
  res.write("data: [DONE]\n\n");
  res.end();
}


Acceptance tests (must pass)

 All OpenAI calls use responses.create or responses.stream exclusively, not chat.completions. Proof: grep shows zero chat.completions.

 Params used match the docs (model, input, streaming pattern). Cite the exact doc line in a code comment near each call. 
OpenAI Platform
+1

 Streaming endpoint yields incremental chunks as per the streaming reference; the UI renders partial text. 
OpenAI Platform

 SDK is imported from openai and instantiated once; API key via env. 
GitHub

Non-negotiables

No hardcoded API keys.

No undocumented params. If a param isn’t in the reference above, don’t invent it. 
OpenAI Platform

Next 3 actions

Drop the Docs Contract into your Replit prompt and commit a “responses-api-enforcement” branch.

Replace any lingering chat.completions usage with responses.create and wire up the streaming endpoint.

Add a pre-commit grep to fail builds if chat.completions appears.

Risks & mitigations

Param drift if the API evolves. Mitigation: keep doc links in comments and re-verify when models change. 
OpenAI Platform

Streaming shape confusion. Mitigation: follow the streaming reference event types verbatim and log unknown events in dev. 
OpenAI Platform
+1